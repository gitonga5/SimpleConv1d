
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c80136",
   "metadata": {},
   "source": [
    "##### 【Problem 1 】 Creating a one-dimensional convolution layer class with limited number of channels to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba002ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3035, Accuracy: 0.1300\n",
      "Epoch 2/10, Loss: 2.2629, Accuracy: 0.2400\n",
      "Epoch 3/10, Loss: 2.1884, Accuracy: 0.3000\n",
      "Epoch 4/10, Loss: 2.0323, Accuracy: 0.5400\n",
      "Epoch 5/10, Loss: 1.6877, Accuracy: 0.7000\n",
      "Epoch 6/10, Loss: 1.1235, Accuracy: 0.8000\n",
      "Epoch 7/10, Loss: 0.6916, Accuracy: 0.8000\n",
      "Epoch 8/10, Loss: 0.5466, Accuracy: 0.8000\n",
      "Epoch 9/10, Loss: 0.5030, Accuracy: 0.8000\n",
      "Epoch 10/10, Loss: 0.4878, Accuracy: 0.8000\n",
      "Test Loss: 2.7514, Test Accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    A one-dimensional convolution layer with a single input and output channel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filter_size : int\n",
    "        The size of the convolutional filter.\n",
    "    initializer : object\n",
    "        An initializer object with a 'W' method for initializing weights (e.g., XavierInitializer).\n",
    "    optimizer : object\n",
    "        An optimizer object with an 'update' method for updating weights and biases (e.g., SGD).\n",
    "    bias : bool\n",
    "        Whether to include a bias term. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size, initializer, optimizer, bias=True):\n",
    "        self.filter_size = filter_size\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.use_bias = bias\n",
    "        self.W = initializer.W(filter_size)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros(1)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.X = None\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for the one-dimensional convolution layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, n_features)\n",
    "            The input data. For this simple implementation, batch_size is assumed to be 1,\n",
    "            so the shape is (1, n_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A : ndarray of shape (1, n_output)\n",
    "            The output of the convolution layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        batch_size, n_features = X.shape\n",
    "        n_output = n_features - self.filter_size + 1\n",
    "        A = np.zeros((batch_size, n_output))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for i in range(n_output):\n",
    "                A[b, i] = np.sum(X[b, i:i + self.filter_size] * self.W)\n",
    "                if self.use_bias:\n",
    "                    A[b, i] += self.b[0]\n",
    "\n",
    "        self.A = A\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Performs backward propagation for the one-dimensional convolution layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (1, n_output)\n",
    "            Gradients of the loss with respect to the output of this layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray of shape (1, n_features)\n",
    "            Gradients of the loss with respect to the input of this layer.\n",
    "        \"\"\"\n",
    "        n_features = self.X.shape[1]\n",
    "        n_output = dA.shape[1]\n",
    "        F = self.filter_size\n",
    "\n",
    "        # Gradient for weights\n",
    "        dW = np.zeros_like(self.W)\n",
    "        for s in range(F):\n",
    "            dW[s] = np.sum(dA[0, :] * self.X[0, s:s + n_output])\n",
    "        self.dW = dW\n",
    "\n",
    "        # Gradient for bias\n",
    "        if self.use_bias:\n",
    "            self.db = np.sum(dA)\n",
    "\n",
    "        # Gradient for the previous layer's input\n",
    "        dX = np.zeros_like(self.X)\n",
    "        for j in range(n_features):\n",
    "            for s in range(F):\n",
    "                if 0 <= j - s < n_output:\n",
    "                    dX[0, j] += dA[0, j - s] * self.W[s]\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using the optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "        \"\"\"\n",
    "        self.W = self.optimizer.update(self.W, self.dW, lr)\n",
    "        if self.use_bias:\n",
    "            self.b = self.optimizer.update(self.b, self.db, lr)\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Initializes weights using the Xavier (or Glorot) initialization method.\n",
    "    \"\"\"\n",
    "    def W(self, *shape):\n",
    "        \"\"\"\n",
    "        Initializes weights with a scale based on the number of input units.\n",
    "        For a 1D convolution layer with a single input channel,\n",
    "        the number of input units is effectively the filter size.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        *shape : tuple\n",
    "            The shape of the weight array to initialize.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The initialized weight array.\n",
    "        \"\"\"\n",
    "        n_in = shape[0]\n",
    "        scale = np.sqrt(1. / n_in)\n",
    "        return np.random.randn(*shape) * scale\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer.\n",
    "    \"\"\"\n",
    "    def update(self, params, grads, lr):\n",
    "        \"\"\"\n",
    "        Updates parameters based on their gradients and the learning rate.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            The parameters to update.\n",
    "        grads : ndarray\n",
    "            The gradients of the loss with respect to the parameters.\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The updated parameters.\n",
    "        \"\"\"\n",
    "        return params - lr * grads\n",
    "\n",
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    A simple 1D Convolutional Neural Network classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filter_size : int\n",
    "        The size of the convolutional filter in the SimpleConv1d layer.\n",
    "    n_neurons : int\n",
    "        The number of neurons in the fully connected layer.\n",
    "    n_output : int\n",
    "        The number of output units (number of classes).\n",
    "    initializer : object\n",
    "        An initializer object for weight initialization.\n",
    "    optimizer : object\n",
    "        An optimizer object for updating weights and biases.\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size, n_neurons, n_output, initializer, optimizer):\n",
    "        self.conv1d = SimpleConv1d(filter_size, initializer, optimizer)\n",
    "        self.fc = self._Dense(n_neurons, initializer, optimizer)\n",
    "        self.out = self._Dense(n_output, initializer, optimizer)\n",
    "\n",
    "    class _Dense:\n",
    "        \"\"\"\n",
    "        A simple fully connected layer.\n",
    "        \"\"\"\n",
    "        def __init__(self, n_nodes, initializer, optimizer, use_bias=True):\n",
    "            self.n_nodes = n_nodes\n",
    "            self.initializer = initializer\n",
    "            self.optimizer = optimizer\n",
    "            self.use_bias = use_bias\n",
    "            self.W = None\n",
    "            self.b = None\n",
    "            self.dW = None\n",
    "            self.db = None\n",
    "            self.X = None\n",
    "            self.Z = None\n",
    "            self.A = None\n",
    "\n",
    "        def forward(self, X):\n",
    "            \"\"\"\n",
    "            Performs forward propagation for the fully connected layer.\n",
    "            \"\"\"\n",
    "            self.X = X\n",
    "            if self.W is None:\n",
    "                self.W = self.initializer.W(X.shape[1], self.n_nodes)\n",
    "                if self.use_bias:\n",
    "                    self.b = np.zeros(self.n_nodes)\n",
    "            self.Z = np.dot(X, self.W)\n",
    "            if self.use_bias:\n",
    "                self.Z += self.b\n",
    "            self.A = self._relu(self.Z)\n",
    "            return self.A\n",
    "\n",
    "        def backward(self, dA):\n",
    "            \"\"\"\n",
    "            Performs backward propagation for the fully connected layer.\n",
    "            \"\"\"\n",
    "            dZ = dA * self._relu_grad(self.Z)\n",
    "            self.dW = np.dot(self.X.T, dZ)\n",
    "            if self.use_bias:\n",
    "                self.db = np.sum(dZ, axis=0)\n",
    "            dX = np.dot(dZ, self.W.T)\n",
    "            return dX\n",
    "\n",
    "        def update(self, lr):\n",
    "            \"\"\"\n",
    "            Updates the weights and biases using the optimizer.\n",
    "            \"\"\"\n",
    "            self.W = self.optimizer.update(self.W, self.dW, lr)\n",
    "            if self.use_bias:\n",
    "                self.b = self.optimizer.update(self.b, self.db, lr)\n",
    "\n",
    "        def _relu(self, x):\n",
    "            return np.maximum(0, x)\n",
    "\n",
    "        def _relu_grad(self, x):\n",
    "            return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation through the 1D CNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : ndarray of shape (batch_size, n_output)\n",
    "            The output probabilities after the softmax layer.\n",
    "        \"\"\"\n",
    "        conv_output = self.conv1d.forward(X)\n",
    "        # Flatten the output of the convolution layer for the fully connected layer\n",
    "        fc_input = conv_output.reshape(X.shape[0], -1)\n",
    "        fc_output = self.fc.forward(fc_input)\n",
    "        output = self._softmax(self.out.forward(fc_output))\n",
    "        return output\n",
    "\n",
    "    def backward(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through the 1D CNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray of shape (batch_size, n_output)\n",
    "            The true labels in one-hot encoded format.\n",
    "        y_pred : ndarray of shape (batch_size, n_output)\n",
    "            The predicted probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Output layer backward\n",
    "        dout = y_pred - y\n",
    "        dfc_output = self.out.backward(dout)\n",
    "\n",
    "        # Fully connected layer backward\n",
    "        dfc_input = self.fc.backward(dfc_output)\n",
    "\n",
    "        # Reshape the gradient to match the convolution output shape\n",
    "        dconv_output = dfc_input.reshape(self.conv1d.A.shape)\n",
    "\n",
    "        # Convolution layer backward\n",
    "        dinput = self.conv1d.backward(dconv_output)\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of all layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "        \"\"\"\n",
    "        self.conv1d.update(lr)\n",
    "        self.fc.update(lr)\n",
    "        self.out.update(lr)\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        \"\"\"\n",
    "        Applies the softmax function to an array.\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates the cross-entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray of shape (batch_size, n_output)\n",
    "            The true labels in one-hot encoded format.\n",
    "        y_pred : ndarray of shape (batch_size, n_output)\n",
    "            The predicted probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average cross-entropy loss.\n",
    "        \"\"\"\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(y * np.log(y_pred + 1e-7)) / batch_size\n",
    "\n",
    "    def accuracy(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray of shape (batch_size, n_output)\n",
    "            The true labels in one-hot encoded format.\n",
    "        y_pred : ndarray of shape (batch_size, n_output)\n",
    "            The predicted probabilities.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy.\n",
    "        \"\"\"\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        y_predicted = np.argmax(y_pred, axis=1)\n",
    "        return np.mean(y_true == y_predicted)\n",
    "\n",
    "# Example usage (requires MNIST dataset loading - not included here for brevity)\n",
    "if __name__ == '__main__':\n",
    "    # Assume you have loaded and preprocessed the MNIST dataset\n",
    "    # For 1D convolution, you might flatten or process the images differently\n",
    "    # Here's a placeholder for demonstration with a dummy dataset\n",
    "\n",
    "    # Example: Flattened MNIST images (28x28 = 784 features)\n",
    "    X_train = np.random.rand(100, 784)\n",
    "    y_train = np.eye(10)[np.random.randint(0, 10, 100)]\n",
    "    X_test = np.random.rand(20, 784)\n",
    "    y_test = np.eye(10)[np.random.randint(0, 10, 20)]\n",
    "\n",
    "    # Hyperparameters\n",
    "    filter_size = 3\n",
    "    n_neurons = 128\n",
    "    n_output = 10\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "    batch_size = 1 # SimpleConv1d currently supports batch_size = 1\n",
    "\n",
    "    # Initialize components\n",
    "    initializer = XavierInitializer()\n",
    "    optimizer = SGD()\n",
    "    model = Scratch1dCNNClassifier(filter_size, n_neurons, n_output, initializer, optimizer)\n",
    "\n",
    "    # Training loop (simplified for demonstration)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        for i in range(X_train.shape[0]):\n",
    "            x_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model.forward(x_batch)\n",
    "\n",
    "            # Calculate loss and accuracy\n",
    "            loss = model.loss(y_batch, y_pred)\n",
    "            accuracy = model.accuracy(y_batch, y_pred)\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "            # Backward pass and update\n",
    "            model.backward(y_batch, y_pred)\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        avg_loss = epoch_loss / X_train.shape[0]\n",
    "        avg_accuracy = epoch_accuracy / X_train.shape[0]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "    # Evaluation (simplified)\n",
    "    y_pred_test = model.forward(X_test)\n",
    "    test_loss = model.loss(y_test, y_pred_test)\n",
    "    test_accuracy = model.accuracy(y_test, y_pred_test)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb2819",
   "metadata": {},
   "source": [
    "##### 【Problem 2 】 Calculation of output size after 1st dimensional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3df0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 100\n",
      "Padding: 2\n",
      "Filter size: 5\n",
      "Stride: 1\n",
      "Output size after 1D convolution: 100\n",
      "\n",
      "Input size: 28\n",
      "Padding: 0\n",
      "Filter size: 3\n",
      "Stride: 2\n",
      "Output size after 1D convolution: 13\n"
     ]
    }
   ],
   "source": [
    "def calculate_conv1d_output_size(input_size, padding, filter_size, stride):\n",
    "    \"\"\"\n",
    "    Calculates the output size (number of feature quantities) after a 1D convolution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The size of the input (number of feature quantities).\n",
    "    padding : int\n",
    "        The number of paddings applied to each end of the input.\n",
    "    filter_size : int\n",
    "        The size of the convolutional filter.\n",
    "    stride : int\n",
    "        The stride of the convolution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The output size after the 1D convolution.\n",
    "    \"\"\"\n",
    "    output_size = ((input_size + 2 * padding - filter_size) // stride) + 1\n",
    "    return output_size\n",
    "\n",
    "# Example usage:\n",
    "input_size = 100\n",
    "padding = 2\n",
    "filter_size = 5\n",
    "stride = 1\n",
    "output_size = calculate_conv1d_output_size(input_size, padding, filter_size, stride)\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Padding: {padding}\")\n",
    "print(f\"Filter size: {filter_size}\")\n",
    "print(f\"Stride: {stride}\")\n",
    "print(f\"Output size after 1D convolution: {output_size}\")\n",
    "\n",
    "input_size = 28\n",
    "padding = 0\n",
    "filter_size = 3\n",
    "stride = 2\n",
    "output_size = calculate_conv1d_output_size(input_size, padding, filter_size, stride)\n",
    "print(f\"\\nInput size: {input_size}\")\n",
    "print(f\"Padding: {padding}\")\n",
    "print(f\"Filter size: {filter_size}\")\n",
    "print(f\"Stride: {stride}\")\n",
    "print(f\"Output size after 1D convolution: {output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e584ef",
   "metadata": {},
   "source": [
    "##### 【Problem 3 】 Experiment of 1D convolution layer in small array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd2b9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Propagation Output (a): [35. 50.]\n",
      "Expected Forward Output (a): [35 50]\n",
      "Forward propagation matches the expected output.\n",
      "\n",
      "Backward Propagation delta_b: 30\n",
      "Expected delta_b: 30\n",
      "delta_b matches the expected value.\n",
      "Backward Propagation delta_w: [ 50  80 110]\n",
      "Expected delta_w: [ 50  80 110]\n",
      "delta_w matches the expected value.\n",
      "Backward Propagation delta_x: [ 30 110 170 140]\n",
      "Expected delta_x: [ 30. 110. 170. 140.]\n",
      "delta_x matches the expected value.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1dTest:\n",
    "    \"\"\"\n",
    "    A one-dimensional convolution layer for testing with a fixed small array.\n",
    "    \"\"\"\n",
    "    def __init__(self, w, b):\n",
    "        self.W = w\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.X = None\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        filter_size = self.W.shape[0]\n",
    "        n_features = X.shape[0]\n",
    "        n_output = n_features - filter_size + 1\n",
    "        A = np.zeros(n_output)\n",
    "\n",
    "        for i in range(n_output):\n",
    "            A[i] = np.sum(X[i:i + filter_size] * self.W) + self.b[0]\n",
    "\n",
    "        self.A = A\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Performs backward propagation.\n",
    "        \"\"\"\n",
    "        n_output = dA.shape[0]\n",
    "        n_features = self.X.shape[0]\n",
    "        filter_size = self.W.shape[0]\n",
    "\n",
    "        # Gradient for bias\n",
    "        self.db = np.sum(dA)\n",
    "\n",
    "        # Gradient for weights\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        for s in range(filter_size):\n",
    "            for i in range(n_output):\n",
    "                self.dW[s] += dA[i] * self.X[i + s]\n",
    "\n",
    "        # Gradient for the previous layer's input\n",
    "        dX = np.zeros_like(self.X)\n",
    "        for j in range(n_features):\n",
    "            for s in range(filter_size):\n",
    "                if 0 <= j - s < n_output:\n",
    "                    dX[j] += dA[j - s] * self.W[s]\n",
    "\n",
    "        return dX\n",
    "\n",
    "# Given input, weight, and bias\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "# Instantiate the SimpleConv1dTest layer\n",
    "conv1d_test = SimpleConv1dTest(w, b)\n",
    "\n",
    "# Forward propagation\n",
    "a = conv1d_test.forward(x)\n",
    "print(f\"Forward Propagation Output (a): {a}\")\n",
    "expected_a = np.array([1*3 + 2*5 + 3*7 + 1, 2*3 + 3*5 + 4*7 + 1])\n",
    "print(f\"Expected Forward Output (a): {expected_a}\")\n",
    "assert np.allclose(a, expected_a), \"Forward propagation mismatch!\"\n",
    "print(\"Forward propagation matches the expected output.\")\n",
    "\n",
    "# Back propagation with given error\n",
    "delta_a = np.array([10, 20])\n",
    "delta_x = conv1d_test.backward(delta_a)\n",
    "delta_w = conv1d_test.dW\n",
    "delta_b = conv1d_test.db\n",
    "\n",
    "print(f\"\\nBackward Propagation delta_b: {delta_b}\")\n",
    "expected_delta_b = np.sum(delta_a)\n",
    "print(f\"Expected delta_b: {expected_delta_b}\")\n",
    "assert np.allclose(delta_b, expected_delta_b), \"delta_b mismatch!\"\n",
    "print(\"delta_b matches the expected value.\")\n",
    "\n",
    "print(f\"Backward Propagation delta_w: {delta_w}\")\n",
    "expected_delta_w = np.array([\n",
    "    delta_a[0] * x[0] + delta_a[1] * x[1],\n",
    "    delta_a[0] * x[1] + delta_a[1] * x[2],\n",
    "    delta_a[0] * x[2] + delta_a[1] * x[3]\n",
    "])\n",
    "print(f\"Expected delta_w: {expected_delta_w}\")\n",
    "assert np.allclose(delta_w, expected_delta_w), \"delta_w mismatch!\"\n",
    "print(\"delta_w matches the expected value.\")\n",
    "\n",
    "print(f\"Backward Propagation delta_x: {delta_x}\")\n",
    "expected_delta_x = np.zeros_like(x, dtype=float)\n",
    "expected_delta_x[0] = delta_a[0] * w[0]\n",
    "expected_delta_x[1] = delta_a[0] * w[1] + delta_a[1] * w[0]\n",
    "expected_delta_x[2] = delta_a[0] * w[2] + delta_a[1] * w[1]\n",
    "expected_delta_x[3] = delta_a[1] * w[2]\n",
    "print(f\"Expected delta_x: {expected_delta_x}\")\n",
    "assert np.allclose(delta_x, expected_delta_x), \"delta_x mismatch!\"\n",
    "print(\"delta_x matches the expected value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea8bc9",
   "metadata": {},
   "source": [
    "##### 【Problem 4 】 Creating a one-dimensional convolution layer class that does not limit the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998c97d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Propagation Output (a):\n",
      " [[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "Expected Forward Output (a):\n",
      " [[16 22]\n",
      " [17 23]\n",
      " [18 24]]\n",
      "Forward propagation matches the expected output.\n",
      "\n",
      "Backward Propagation (implemented) delta_b: [2. 2. 2.]\n",
      "Backward Propagation (implemented) delta_w:\n",
      " [[[3. 5. 7.]\n",
      "  [5. 7. 9.]]\n",
      "\n",
      " [[3. 5. 7.]\n",
      "  [5. 7. 9.]]\n",
      "\n",
      " [[3. 5. 7.]\n",
      "  [5. 7. 9.]]]\n",
      "Backward Propagation (implemented) delta_x:\n",
      " [[3 6 6 3]\n",
      " [3 6 6 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DummyInitializer:\n",
    "    \"\"\"A dummy initializer for testing.\"\"\"\n",
    "    def W(self, *shape):\n",
    "        return np.ones(shape) if shape == (3, 2, 3) else np.zeros(shape)\n",
    "\n",
    "class DummyOptimizer:\n",
    "    \"\"\"A dummy optimizer for testing.\"\"\"\n",
    "    def update(self, params, grads, lr):\n",
    "        return params - lr * grads\n",
    "\n",
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    A one-dimensional convolution layer that handles multiple input and output channels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels.\n",
    "    out_channels : int\n",
    "        Number of output channels.\n",
    "    filter_size : int\n",
    "        Size of the convolutional filter.\n",
    "    initializer : object\n",
    "        Initializer object (e.g., XavierInitializer).\n",
    "    optimizer : object\n",
    "        Optimizer object (e.g., SGD).\n",
    "    stride : int\n",
    "        Stride of the convolution. Default is 1.\n",
    "    padding : int\n",
    "        Padding applied to both ends of the input. Default is 0.\n",
    "    bias : bool\n",
    "        Whether to use bias. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, filter_size, initializer, optimizer,\n",
    "                 stride=1, padding=0, bias=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.use_bias = bias\n",
    "        self.W = initializer.W(out_channels, in_channels, filter_size)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros(out_channels)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.X = None\n",
    "        self.A = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (in_channels, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A : ndarray of shape (out_channels, n_output)\n",
    "            Output of the convolution layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        n_in_channels, n_features = X.shape\n",
    "        n_output = (n_features + 2 * self.padding - self.filter_size) // self.stride + 1\n",
    "        A = np.zeros((self.out_channels, n_output))\n",
    "\n",
    "        # Pad the input\n",
    "        if self.padding > 0:\n",
    "            padded_X = np.pad(X, ((0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            padded_X = X\n",
    "\n",
    "        for out_c in range(self.out_channels):\n",
    "            for i in range(n_output):\n",
    "                start = i * self.stride\n",
    "                end = start + self.filter_size\n",
    "                # Element-wise multiplication across input channels and filter\n",
    "                conv_sum = np.sum(padded_X[:, start:end] * self.W[out_c, :, :])\n",
    "                if self.use_bias:\n",
    "                    A[out_c, i] = conv_sum + self.b[out_c]\n",
    "                else:\n",
    "                    A[out_c, i] = conv_sum\n",
    "\n",
    "        self.A = A\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Performs backward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (out_channels, n_output)\n",
    "            Gradients of the loss with respect to the output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray of shape (in_channels, n_features)\n",
    "            Gradients of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        n_in_channels, n_features = self.X.shape\n",
    "        n_output = dA.shape[1]\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        dX = np.zeros_like(self.X)\n",
    "\n",
    "        # Gradient for bias\n",
    "        if self.use_bias:\n",
    "            self.db = np.sum(dA, axis=1)\n",
    "\n",
    "        # Gradient for weights\n",
    "        if self.padding > 0:\n",
    "            padded_X = np.pad(self.X, ((0, 0), (self.padding, self.padding)), mode='constant')\n",
    "        else:\n",
    "            padded_X = self.X\n",
    "\n",
    "        for out_c in range(self.out_channels):\n",
    "            for in_c in range(self.in_channels):\n",
    "                for s in range(self.filter_size):\n",
    "                    for i in range(n_output):\n",
    "                        start = i * self.stride\n",
    "                        end = start + self.filter_size\n",
    "                        if start + s < padded_X.shape[1]:\n",
    "                            self.dW[out_c, in_c, s] += dA[out_c, i] * padded_X[in_c, start + s]\n",
    "\n",
    "        # Gradient for input\n",
    "        padded_dX = np.zeros_like(padded_X)\n",
    "        for out_c in range(self.out_channels):\n",
    "            for in_c in range(self.in_channels):\n",
    "                for s in range(self.filter_size):\n",
    "                    for i in range(n_output):\n",
    "                        input_index = i * self.stride + s\n",
    "                        if 0 <= input_index < padded_dX.shape[1]:\n",
    "                            padded_dX[in_c, input_index] += dA[out_c, i] * self.W[out_c, in_c, self.filter_size - 1 - s]\n",
    "\n",
    "        # Remove padding from dX\n",
    "        if self.padding > 0:\n",
    "            dX = padded_dX[:, self.padding:-self.padding]\n",
    "        else:\n",
    "            dX = padded_dX\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using the optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "        \"\"\"\n",
    "        self.W = self.optimizer.update(self.W, self.dW, lr)\n",
    "        if self.use_bias:\n",
    "            self.b = self.optimizer.update(self.b, self.db, lr)\n",
    "\n",
    "# Example Usage:\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])  # shape(2, 4) - (in_channels, n_features)\n",
    "w = np.ones((3, 2, 3))  # shape(3, 2, 3) - (out_channels, in_channels, filter_size)\n",
    "b = np.array([1, 2, 3])  # shape(3,) - (out_channels)\n",
    "\n",
    "initializer = DummyInitializer()\n",
    "optimizer = DummyOptimizer()\n",
    "\n",
    "conv1d_layer = Conv1d(in_channels=2, out_channels=3, filter_size=3,\n",
    "                     initializer=initializer, optimizer=optimizer, bias=True)\n",
    "\n",
    "# Set the weights and biases to the example values for forward pass verification\n",
    "conv1d_layer.W = w\n",
    "conv1d_layer.b = b\n",
    "\n",
    "a = conv1d_layer.forward(x)\n",
    "print(\"Forward Propagation Output (a):\\n\", a)\n",
    "expected_a = np.array([[16, 22], [17, 23], [18, 24]])\n",
    "print(\"Expected Forward Output (a):\\n\", expected_a)\n",
    "assert np.allclose(a, expected_a), \"Forward propagation mismatch!\"\n",
    "print(\"Forward propagation matches the expected output.\")\n",
    "\n",
    "# Conceptual Backpropagation (Hand Calculation):\n",
    "# ... (rest of the conceptual backpropagation remains the same) ...\n",
    "\n",
    "# You can uncomment the following to test the backward pass implementation\n",
    "delta_a = np.ones((3, 2))  # Shape (out_channels, n_output)\n",
    "dX = conv1d_layer.backward(delta_a)\n",
    "print(\"\\nBackward Propagation (implemented) delta_b:\", conv1d_layer.db)\n",
    "print(\"Backward Propagation (implemented) delta_w:\\n\", conv1d_layer.dW)\n",
    "print(\"Backward Propagation (implemented) delta_x:\\n\", dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5f04e",
   "metadata": {},
   "source": [
    "##### 【Problem 5 】 (Advance Challenge) Padding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb526921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX with integer padding:\n",
      " [[5 5 5 5 5]]\n",
      "dX with 'same' padding (stride 1):\n",
      " [[2 3 3 3 2]]\n",
      "dX with 'same' padding (stride 2):\n",
      " [[1 2 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DummyInitializer:\n",
    "    \"\"\"A dummy initializer for testing.\"\"\"\n",
    "    def W(self, *shape):\n",
    "        return np.ones(shape) if shape == (3, 2, 3) else np.zeros(shape)\n",
    "\n",
    "class DummyOptimizer:\n",
    "    \"\"\"A dummy optimizer for testing.\"\"\"\n",
    "    def update(self, params, grads, lr):\n",
    "        return params - lr * grads\n",
    "\n",
    "class Conv1dWithPadding:\n",
    "    \"\"\"\n",
    "    A one-dimensional convolution layer with padding and the option to maintain\n",
    "    the original input size for the output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels.\n",
    "    out_channels : int\n",
    "        Number of output channels.\n",
    "    filter_size : int\n",
    "        Size of the convolutional filter.\n",
    "    initializer : object\n",
    "        Initializer object.\n",
    "    optimizer : object\n",
    "        Optimizer object.\n",
    "    stride : int\n",
    "        Stride of the convolution. Default is 1.\n",
    "    padding : int or str\n",
    "        Number of padding units on both ends of the input.\n",
    "        If 'same', padding is calculated to maintain the input size. Default is 0.\n",
    "    bias : bool\n",
    "        Whether to use bias. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, filter_size, initializer, optimizer,\n",
    "                 stride=1, padding=0, bias=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.use_bias = bias\n",
    "        self.W = initializer.W(out_channels, in_channels, filter_size)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros(out_channels)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.X = None\n",
    "        self.A = None\n",
    "        self.padded_X = None  # Store padded input for backward pass\n",
    "\n",
    "    def _calculate_padding(self, input_size, filter_size, stride, output_size=None):\n",
    "        \"\"\"\n",
    "        Calculates the padding needed to achieve a desired output size.\n",
    "        Used for 'same' padding.\n",
    "        \"\"\"\n",
    "        if output_size is None:\n",
    "            output_size = np.ceil(input_size / stride).astype(int)\n",
    "        padding_needed = (output_size - 1) * stride + filter_size - input_size\n",
    "        padding_before = padding_needed // 2\n",
    "        padding_after = padding_needed - padding_before\n",
    "        return padding_before, padding_after\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation with padding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (in_channels, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A : ndarray of shape (out_channels, n_output)\n",
    "            Output of the convolution layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        n_in_channels, n_features = X.shape\n",
    "\n",
    "        if isinstance(self.padding, int):\n",
    "            padding_width = ((0, 0), (self.padding, self.padding))\n",
    "        elif self.padding == 'same':\n",
    "            padding_before, padding_after = self._calculate_padding(n_features, self.filter_size, self.stride)\n",
    "            padding_width = ((0, 0), (padding_before, padding_after))\n",
    "        else:\n",
    "            raise ValueError(\"Padding must be an integer or 'same'\")\n",
    "\n",
    "        self.padded_X = np.pad(X, padding_width, mode='constant')\n",
    "        padded_n_features = self.padded_X.shape[1]\n",
    "        n_output = (padded_n_features - self.filter_size) // self.stride + 1\n",
    "        A = np.zeros((self.out_channels, n_output))\n",
    "\n",
    "        for out_c in range(self.out_channels):\n",
    "            for i in range(n_output):\n",
    "                start = i * self.stride\n",
    "                end = start + self.filter_size\n",
    "                # Element-wise multiplication across input channels and filter\n",
    "                conv_sum = np.sum(self.padded_X[:, start:end] * self.W[out_c, :, :])\n",
    "                if self.use_bias:\n",
    "                    A[out_c, i] = conv_sum + self.b[out_c]\n",
    "                else:\n",
    "                    A[out_c, i] = conv_sum\n",
    "\n",
    "        self.A = A\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Performs backward propagation with padding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (out_channels, n_output)\n",
    "            Gradients of the loss with respect to the output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray of shape (in_channels, n_features)\n",
    "            Gradients of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        if self.padded_X is None:\n",
    "            raise ValueError(\"Forward pass must be performed before backward pass.\")\n",
    "\n",
    "        n_in_channels, padded_n_features = self.padded_X.shape\n",
    "        n_output = dA.shape[1]\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        padded_dX = np.zeros_like(self.padded_X)\n",
    "\n",
    "        # Gradient for bias\n",
    "        if self.use_bias:\n",
    "            self.db = np.sum(dA, axis=1)\n",
    "\n",
    "        # Gradient for weights\n",
    "        for out_c in range(self.out_channels):\n",
    "            for in_c in range(self.in_channels):\n",
    "                for s in range(self.filter_size):\n",
    "                    for i in range(n_output):\n",
    "                        start = i * self.stride\n",
    "                        end = start + self.filter_size\n",
    "                        if start + s < padded_n_features:\n",
    "                            self.dW[out_c, in_c, s] += dA[out_c, i] * self.padded_X[in_c, start + s]\n",
    "\n",
    "        # Gradient for input (on the padded input)\n",
    "        for out_c in range(self.out_channels):\n",
    "            for in_c in range(self.in_channels):\n",
    "                for s in range(self.filter_size):\n",
    "                    for i in range(n_output):\n",
    "                        input_index = i * self.stride + s\n",
    "                        if 0 <= input_index < padded_dX.shape[1]:\n",
    "                            padded_dX[in_c, input_index] += dA[out_c, i] * self.W[out_c, in_c, self.filter_size - 1 - s]\n",
    "\n",
    "        # Remove padding from dX\n",
    "        if isinstance(self.padding, int) and self.padding > 0:\n",
    "            dX = padded_dX[:, self.padding:-self.padding]\n",
    "        elif self.padding == 'same':\n",
    "            padding_before, padding_after = self._calculate_padding(self.X.shape[1], self.filter_size, self.stride, output_size=self.A.shape[1])\n",
    "            dX = padded_dX[:, padding_before:padded_n_features - padding_after]\n",
    "        else:\n",
    "            dX = padded_dX\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using the optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "        \"\"\"\n",
    "        self.W = self.optimizer.update(self.W, self.dW, lr)\n",
    "        if self.use_bias:\n",
    "            self.b = self.optimizer.update(self.b, self.db, lr)\n",
    "\n",
    "# Example Usage:\n",
    "x = np.array([[1, 2, 3, 4, 5]])  # Single input channel for simplicity\n",
    "w_int_pad = np.array([[[2, 3]]])\n",
    "w_same_pad = np.array([[[1, 1, 1]]])\n",
    "b = np.array([0])\n",
    "\n",
    "initializer = DummyInitializer()\n",
    "optimizer = DummyOptimizer()\n",
    "\n",
    "# Test zero padding\n",
    "conv1d_pad_int = Conv1dWithPadding(in_channels=1, out_channels=1, filter_size=2,\n",
    "                                   initializer=initializer, optimizer=optimizer, padding=1)\n",
    "conv1d_pad_int.W = w_int_pad\n",
    "conv1d_pad_int.b = b\n",
    "a_pad_int = conv1d_pad_int.forward(x)\n",
    "expected_a_pad_int = np.array([[3, 8, 13, 18, 23, 10]])\n",
    "assert np.allclose(a_pad_int, expected_a_pad_int), \"Integer padding forward mismatch!\"\n",
    "\n",
    "# Test 'same' padding (stride 1)\n",
    "conv1d_pad_same_stride1 = Conv1dWithPadding(in_channels=1, out_channels=1, filter_size=3,\n",
    "                                    initializer=initializer, optimizer=optimizer, padding='same', stride=1)\n",
    "conv1d_pad_same_stride1.W = w_same_pad\n",
    "conv1d_pad_same_stride1.b = b\n",
    "a_pad_same_stride1 = conv1d_pad_same_stride1.forward(x)\n",
    "expected_a_pad_same_stride1 = np.array([[3, 6, 9, 12, 9]])\n",
    "assert np.allclose(a_pad_same_stride1, expected_a_pad_same_stride1), \"'Same' padding forward mismatch (stride 1)!\"\n",
    "\n",
    "# Test 'same' padding (stride 2)\n",
    "conv1d_pad_same_stride2 = Conv1dWithPadding(in_channels=1, out_channels=1, filter_size=3,\n",
    "                                         initializer=initializer, optimizer=optimizer, padding='same', stride=2)\n",
    "conv1d_pad_same_stride2.W = w_same_pad\n",
    "conv1d_pad_same_stride2.b = b\n",
    "a_pad_same_stride2 = conv1d_pad_same_stride2.forward(x)\n",
    "expected_a_pad_same_stride2 = np.array([[3, 9, 9]])\n",
    "assert np.allclose(a_pad_same_stride2, expected_a_pad_same_stride2), \"'Same' padding forward mismatch (stride 2)!\"\n",
    "\n",
    "# Example of backward pass (basic test - shape consistency)\n",
    "delta_a_pad_int = np.ones_like(a_pad_int)\n",
    "dX_pad_int = conv1d_pad_int.backward(delta_a_pad_int)\n",
    "print(\"dX with integer padding:\\n\", dX_pad_int)\n",
    "assert dX_pad_int.shape == x.shape, \"dX shape mismatch with integer padding!\"\n",
    "\n",
    "delta_a_pad_same_stride1 = np.ones_like(a_pad_same_stride1)\n",
    "dX_pad_same_stride1 = conv1d_pad_same_stride1.backward(delta_a_pad_same_stride1)\n",
    "print(\"dX with 'same' padding (stride 1):\\n\", dX_pad_same_stride1)\n",
    "assert dX_pad_same_stride1.shape == x.shape, \"dX shape mismatch with 'same' padding (stride 1)!\"\n",
    "\n",
    "delta_a_pad_same_stride2 = np.ones_like(a_pad_same_stride2)\n",
    "dX_pad_same_stride2 = conv1d_pad_same_stride2.backward(delta_a_pad_same_stride2)\n",
    "print(\"dX with 'same' padding (stride 2):\\n\", dX_pad_same_stride2)\n",
    "assert dX_pad_same_stride2.shape == x.shape, \"dX shape mismatch with 'same' padding (stride 2)!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d2250",
   "metadata": {},
   "source": [
    "##### 【Problem 6 】 (Advance Challenge) Response to mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fc2b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Propagation Output (mini-batch):\n",
      " [[[ 3.  6.  9. 12.  9.]]\n",
      "\n",
      " [[13. 21. 24. 27. 19.]]]\n",
      "Mini-batch forward propagation matches the expected output.\n",
      "Backward Propagation dX (mini-batch):\n",
      " [[[2 3 3 3 2]]\n",
      "\n",
      " [[2 3 3 3 2]]]\n",
      "Mini-batch backward propagation dX shape is correct.\n",
      "Backward Propagation dW (mini-batch):\n",
      " [[[20.  27.5 24. ]]]\n",
      "Backward Propagation db (mini-batch):\n",
      " [10.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DummyInitializer:\n",
    "    \"\"\"A dummy initializer for testing.\"\"\"\n",
    "    def W(self, *shape):\n",
    "        return np.ones(shape) if shape == (3, 2, 3) else np.zeros(shape)\n",
    "\n",
    "class DummyOptimizer:\n",
    "    \"\"\"A dummy optimizer for testing.\"\"\"\n",
    "    def update(self, params, grads, lr):\n",
    "        return params - lr * grads\n",
    "\n",
    "class Conv1dMiniBatch:\n",
    "    \"\"\"\n",
    "    A one-dimensional convolution layer that handles multiple input and output channels\n",
    "    and mini-batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels.\n",
    "    out_channels : int\n",
    "        Number of output channels.\n",
    "    filter_size : int\n",
    "        Size of the convolutional filter.\n",
    "    initializer : object\n",
    "        Initializer object.\n",
    "    optimizer : object\n",
    "        Optimizer object.\n",
    "    stride : int\n",
    "        Stride of the convolution. Default is 1.\n",
    "    padding : int or str\n",
    "        Number of padding units on both ends of the input.\n",
    "        If 'same', padding is calculated to maintain the input size. Default is 0.\n",
    "    bias : bool\n",
    "        Whether to use bias. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, filter_size, initializer, optimizer,\n",
    "                 stride=1, padding=0, bias=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.use_bias = bias\n",
    "        self.W = initializer.W(out_channels, in_channels, filter_size)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros(out_channels)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.X = None  # Shape (batch_size, in_channels, n_features)\n",
    "        self.A = None  # Shape (batch_size, out_channels, n_output)\n",
    "        self.padded_X = None # Shape (batch_size, in_channels, padded_n_features)\n",
    "\n",
    "    def _calculate_padding(self, input_size, filter_size, stride, output_size=None):\n",
    "        \"\"\"\n",
    "        Calculates the padding needed to achieve a desired output size.\n",
    "        Used for 'same' padding.\n",
    "        \"\"\"\n",
    "        if output_size is None:\n",
    "            output_size = np.ceil(input_size / stride).astype(int)\n",
    "        padding_needed = (output_size - 1) * stride + filter_size - input_size\n",
    "        padding_before = padding_needed // 2\n",
    "        padding_after = padding_needed - padding_before\n",
    "        return padding_before, padding_after\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a mini-batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, in_channels, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A : ndarray of shape (batch_size, out_channels, n_output)\n",
    "            Output of the convolution layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        batch_size, n_in_channels, n_features = X.shape\n",
    "\n",
    "        if isinstance(self.padding, int):\n",
    "            padding_width = ((0, 0), (0, 0), (self.padding, self.padding))\n",
    "        elif self.padding == 'same':\n",
    "            padding_before, padding_after = self._calculate_padding(n_features, self.filter_size, self.stride)\n",
    "            padding_width = ((0, 0), (0, 0), (padding_before, padding_after))\n",
    "        else:\n",
    "            raise ValueError(\"Padding must be an integer or 'same'\")\n",
    "\n",
    "        self.padded_X = np.pad(X, padding_width, mode='constant')\n",
    "        _, _, padded_n_features = self.padded_X.shape\n",
    "        n_output = (padded_n_features - self.filter_size) // self.stride + 1\n",
    "        A = np.zeros((batch_size, self.out_channels, n_output))\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            for out_c in range(self.out_channels):\n",
    "                for i in range(n_output):\n",
    "                    start = i * self.stride\n",
    "                    end = start + self.filter_size\n",
    "                    # Element-wise multiplication across input channels and filter\n",
    "                    conv_sum = np.sum(self.padded_X[batch, :, start:end] * self.W[out_c, :, :])\n",
    "                    if self.use_bias:\n",
    "                        A[batch, out_c, i] = conv_sum + self.b[out_c]\n",
    "                    else:\n",
    "                        A[batch, out_c, i] = conv_sum\n",
    "\n",
    "        self.A = A\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Performs backward propagation for a mini-batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, out_channels, n_output)\n",
    "            Gradients of the loss with respect to the output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray of shape (batch_size, in_channels, n_features)\n",
    "            Gradients of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        if self.padded_X is None:\n",
    "            raise ValueError(\"Forward pass must be performed before backward pass.\")\n",
    "\n",
    "        batch_size, n_in_channels, padded_n_features = self.padded_X.shape\n",
    "        _, n_out_channels, n_output = dA.shape\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        padded_dX = np.zeros_like(self.padded_X)\n",
    "\n",
    "        # Gradient for bias (sum over the batch)\n",
    "        if self.use_bias:\n",
    "            self.db = np.sum(dA, axis=(0, 2))\n",
    "\n",
    "        # Gradient for weights (sum over the batch)\n",
    "        for batch in range(batch_size):\n",
    "            for out_c in range(n_out_channels):\n",
    "                for in_c in range(n_in_channels):\n",
    "                    for s in range(self.filter_size):\n",
    "                        for i in range(n_output):\n",
    "                            start = i * self.stride\n",
    "                            end = start + self.filter_size\n",
    "                            if start + s < padded_n_features:\n",
    "                                self.dW[out_c, in_c, s] += dA[batch, out_c, i] * self.padded_X[batch, in_c, start + s]\n",
    "        self.dW /= batch_size # Average over the batch\n",
    "\n",
    "        # Gradient for input (on the padded input)\n",
    "        for batch in range(batch_size):\n",
    "            for out_c in range(n_out_channels):\n",
    "                for in_c in range(n_in_channels):\n",
    "                    for s in range(self.filter_size):\n",
    "                        for i in range(n_output):\n",
    "                            input_index = i * self.stride + s\n",
    "                            if 0 <= input_index < padded_dX.shape[2]:\n",
    "                                padded_dX[batch, in_c, input_index] += dA[batch, out_c, i] * self.W[out_c, in_c, self.filter_size - 1 - s]\n",
    "\n",
    "        # Remove padding from dX\n",
    "        if isinstance(self.padding, int) and self.padding > 0:\n",
    "            dX = padded_dX[:, :, self.padding:-self.padding]\n",
    "        elif self.padding == 'same':\n",
    "            padding_before, padding_after = self._calculate_padding(self.X.shape[2], self.filter_size, self.stride, output_size=self.A.shape[2])\n",
    "            dX = padded_dX[:, :, padding_before:padded_n_features - padding_after]\n",
    "        else:\n",
    "            dX = padded_dX\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using the optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "        \"\"\"\n",
    "        self.W = self.optimizer.update(self.W, self.dW, lr)\n",
    "        if self.use_bias:\n",
    "            self.b = self.optimizer.update(self.b, self.db, lr)\n",
    "\n",
    "# Example Usage:\n",
    "batch_size = 2\n",
    "in_channels = 1\n",
    "n_features = 5\n",
    "out_channels = 1\n",
    "filter_size = 3\n",
    "\n",
    "x_mini_batch = np.array([[[1, 2, 3, 4, 5]], [[6, 7, 8, 9, 10]]]) # shape (2, 1, 5)\n",
    "w_mini_batch = np.ones((out_channels, in_channels, filter_size)) # shape (1, 1, 3)\n",
    "b_mini_batch = np.array([0]) # shape (1,)\n",
    "\n",
    "initializer = DummyInitializer()\n",
    "optimizer = DummyOptimizer()\n",
    "\n",
    "conv1d_mini_batch = Conv1dMiniBatch(in_channels=in_channels, out_channels=out_channels, filter_size=filter_size,\n",
    "                                     initializer=initializer, optimizer=optimizer, padding='same', stride=1)\n",
    "conv1d_mini_batch.W = w_mini_batch\n",
    "conv1d_mini_batch.b = b_mini_batch\n",
    "\n",
    "a_mini_batch = conv1d_mini_batch.forward(x_mini_batch)\n",
    "print(\"Forward Propagation Output (mini-batch):\\n\", a_mini_batch)\n",
    "# Expected output shape (2, 1, 5) with 'same' padding and stride 1\n",
    "expected_a_mini_batch = np.array([[[3., 6., 9., 12., 9.]], [[13., 21., 24., 27., 19.]]])\n",
    "assert np.allclose(a_mini_batch, expected_a_mini_batch), \"Mini-batch forward mismatch!\"\n",
    "print(\"Mini-batch forward propagation matches the expected output.\")\n",
    "\n",
    "delta_a_mini_batch = np.ones_like(a_mini_batch)\n",
    "dX_mini_batch = conv1d_mini_batch.backward(delta_a_mini_batch)\n",
    "print(\"Backward Propagation dX (mini-batch):\\n\", dX_mini_batch)\n",
    "assert dX_mini_batch.shape == x_mini_batch.shape, \"Mini-batch dX shape mismatch!\"\n",
    "print(\"Mini-batch backward propagation dX shape is correct.\")\n",
    "print(\"Backward Propagation dW (mini-batch):\\n\", conv1d_mini_batch.dW)\n",
    "print(\"Backward Propagation db (mini-batch):\\n\", conv1d_mini_batch.db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9461469",
   "metadata": {},
   "source": [
    "##### 【Problem 7 】 (Advance Challenge) Any number of strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a44139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward with stride 2 ('same' padding):\n",
      " [[[ 3.  9. 15. 13.]]\n",
      "\n",
      " [[17. 30. 36. 27.]]]\n",
      "Stride 2 'same' forward matches expected.\n",
      "Forward with stride 3 (integer padding=1):\n",
      " [[[ 3. 12. 13.]]\n",
      "\n",
      " [[17. 33. 27.]]]\n",
      "Stride 3 integer padding forward matches expected.\n",
      "Backward dX with stride 2 ('same'):\n",
      " (2, 1, 7)\n",
      "Backward dX with stride 3 (integer padding):\n",
      " (2, 1, 7)\n",
      "Backward dW (stride 2):\n",
      " [[[22.5 30.  22.5]]]\n",
      "Backward db (stride 2):\n",
      " [8.]\n",
      "Backward dW (stride 3):\n",
      " [[[16.  22.5 14. ]]]\n",
      "Backward db (stride 3):\n",
      " [6.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DummyInitializer:\n",
    "    \"\"\"A dummy initializer for testing.\"\"\"\n",
    "    def W(self, *shape):\n",
    "        return np.ones(shape) if shape == (3, 2, 3) else np.zeros(shape)\n",
    "\n",
    "class DummyOptimizer:\n",
    "    \"\"\"A dummy optimizer for testing.\"\"\"\n",
    "    def update(self, params, grads, lr):\n",
    "        return params - lr * grads\n",
    "\n",
    "class Conv1dStrideAny:\n",
    "    \"\"\"\n",
    "    A one-dimensional convolution layer that handles multiple input and output channels,\n",
    "    mini-batches, and any stride value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Number of input channels.\n",
    "    out_channels : int\n",
    "        Number of output channels.\n",
    "    filter_size : int\n",
    "        Size of the convolutional filter.\n",
    "    initializer : object\n",
    "        Initializer object.\n",
    "    optimizer : object\n",
    "        Optimizer object.\n",
    "    stride : int\n",
    "        Stride of the convolution. Default is 1.\n",
    "    padding : int or str\n",
    "        Number of padding units on both ends of the input.\n",
    "        If 'same', padding is calculated to maintain the input size (approximately). Default is 0.\n",
    "    bias : bool\n",
    "        Whether to use bias. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, filter_size, initializer, optimizer,\n",
    "                 stride=1, padding=0, bias=True):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.use_bias = bias\n",
    "        self.W = initializer.W(out_channels, in_channels, filter_size)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros(out_channels)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.X = None  # Shape (batch_size, in_channels, n_features)\n",
    "        self.A = None  # Shape (batch_size, out_channels, n_output)\n",
    "        self.padded_X = None # Shape (batch_size, in_channels, padded_n_features)\n",
    "\n",
    "    def _calculate_padding(self, input_size, filter_size, stride, output_size=None):\n",
    "        \"\"\"\n",
    "        Calculates the padding needed to achieve a desired output size.\n",
    "        Used for 'same' padding.\n",
    "        \"\"\"\n",
    "        if output_size is None:\n",
    "            output_size = np.ceil(input_size / stride).astype(int)\n",
    "        padding_needed = (output_size - 1) * stride + filter_size - input_size\n",
    "        padding_before = padding_needed // 2\n",
    "        padding_after = padding_needed - padding_before\n",
    "        return padding_before, padding_after\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a mini-batch with any stride.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, in_channels, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        A : ndarray of shape (batch_size, out_channels, n_output)\n",
    "            Output of the convolution layer.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        batch_size, n_in_channels, n_features = X.shape\n",
    "\n",
    "        if isinstance(self.padding, int):\n",
    "            padding_width = ((0, 0), (0, 0), (self.padding, self.padding))\n",
    "        elif self.padding == 'same':\n",
    "            padding_before, padding_after = self._calculate_padding(n_features, self.filter_size, self.stride)\n",
    "            padding_width = ((0, 0), (0, 0), (padding_before, padding_after))\n",
    "        else:\n",
    "            raise ValueError(\"Padding must be an integer or 'same'\")\n",
    "\n",
    "        self.padded_X = np.pad(X, padding_width, mode='constant')\n",
    "        _, _, padded_n_features = self.padded_X.shape\n",
    "        n_output = (padded_n_features - self.filter_size) // self.stride + 1\n",
    "        A = np.zeros((batch_size, self.out_channels, n_output))\n",
    "\n",
    "        for batch in range(batch_size):\n",
    "            for out_c in range(self.out_channels):\n",
    "                for i in range(n_output):\n",
    "                    start = i * self.stride\n",
    "                    end = start + self.filter_size\n",
    "                    # Element-wise multiplication across input channels and filter\n",
    "                    conv_sum = np.sum(self.padded_X[batch, :, start:end] * self.W[out_c, :, :])\n",
    "                    if self.use_bias:\n",
    "                        A[batch, out_c, i] = conv_sum + self.b[out_c]\n",
    "                    else:\n",
    "                        A[batch, out_c, i] = conv_sum\n",
    "\n",
    "        self.A = A\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Performs backward propagation for a mini-batch with any stride.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, out_channels, n_output)\n",
    "            Gradients of the loss with respect to the output.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dX : ndarray of shape (batch_size, in_channels, n_features)\n",
    "            Gradients of the loss with respect to the input.\n",
    "        \"\"\"\n",
    "        if self.padded_X is None:\n",
    "            raise ValueError(\"Forward pass must be performed before backward pass.\")\n",
    "\n",
    "        batch_size, n_in_channels, padded_n_features = self.padded_X.shape\n",
    "        _, n_out_channels, n_output = dA.shape\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        padded_dX = np.zeros_like(self.padded_X)\n",
    "\n",
    "        # Gradient for bias (sum over the batch and output features)\n",
    "        if self.use_bias:\n",
    "            self.db = np.sum(dA, axis=(0, 2))\n",
    "\n",
    "        # Gradient for weights (sum over the batch and output features)\n",
    "        for batch in range(batch_size):\n",
    "            for out_c in range(n_out_channels):\n",
    "                for in_c in range(n_in_channels):\n",
    "                    for s in range(self.filter_size):\n",
    "                        for i in range(n_output):\n",
    "                            input_index = i * self.stride + s\n",
    "                            if input_index < padded_n_features:\n",
    "                                self.dW[out_c, in_c, s] += dA[batch, out_c, i] * self.padded_X[batch, in_c, input_index]\n",
    "        self.dW /= batch_size # Average over the batch\n",
    "\n",
    "        # Gradient for input (on the padded input)\n",
    "        for batch in range(batch_size):\n",
    "            for in_c in range(n_in_channels):\n",
    "                for j in range(padded_n_features):\n",
    "                    for out_c in range(n_out_channels):\n",
    "                        for s in range(self.filter_size):\n",
    "                            output_index = (j - s) // self.stride\n",
    "                            if (j - s) % self.stride == 0 and 0 <= output_index < n_output:\n",
    "                                padded_dX[batch, in_c, j] += dA[batch, out_c, output_index] * self.W[out_c, in_c, s]\n",
    "\n",
    "        # Remove padding from dX\n",
    "        if isinstance(self.padding, int) and self.padding > 0:\n",
    "            dX = padded_dX[:, :, self.padding:-self.padding]\n",
    "        elif self.padding == 'same':\n",
    "            padding_before, padding_after = self._calculate_padding(self.X.shape[2], self.filter_size, self.stride, output_size=self.A.shape[2])\n",
    "            dX = padded_dX[:, :, padding_before:padded_n_features - padding_after]\n",
    "        else:\n",
    "            dX = padded_dX\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases using the optimizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            The learning rate.\n",
    "        \"\"\"\n",
    "        self.W = self.optimizer.update(self.W, self.dW, lr)\n",
    "        if self.use_bias:\n",
    "            self.b = self.optimizer.update(self.b, self.db, lr)\n",
    "\n",
    "# Example Usage with different strides:\n",
    "batch_size = 2\n",
    "in_channels = 1\n",
    "n_features = 7\n",
    "out_channels = 1\n",
    "filter_size = 3\n",
    "\n",
    "x_stride_any = np.array([[[1, 2, 3, 4, 5, 6, 7]], [[8, 9, 10, 11, 12, 13, 14]]])\n",
    "w_stride_any = np.ones((out_channels, in_channels, filter_size))\n",
    "b_stride_any = np.array([0])\n",
    "\n",
    "initializer = DummyInitializer()\n",
    "optimizer = DummyOptimizer()\n",
    "\n",
    "# Test stride = 2 with 'same' padding\n",
    "conv1d_stride2_same = Conv1dStrideAny(in_channels=in_channels, out_channels=out_channels, filter_size=filter_size,\n",
    "                                       initializer=initializer, optimizer=optimizer, padding='same', stride=2)\n",
    "conv1d_stride2_same.W = w_stride_any\n",
    "conv1d_stride2_same.b = b_stride_any\n",
    "a_stride2_same = conv1d_stride2_same.forward(x_stride_any)\n",
    "print(\"Forward with stride 2 ('same' padding):\\n\", a_stride2_same)\n",
    "# Expected output size: ceil(7/2) = 4\n",
    "# Padded input (total pad 2): [[0, 1, 2, 3, 4, 5, 6, 7, 0]], [[0, 8, 9, 10, 11, 12, 13, 14, 0]]\n",
    "# Output 1: [0+1+2, 2+3+4, 4+5+6, 6+7+0] = [3, 9, 15, 13]\n",
    "# Output 2: [0+8+9, 9+10+11, 11+12+13, 13+14+0] = [17, 30, 36, 27]\n",
    "expected_a_stride2_same = np.array([[[3., 9., 15., 13.]], [[17., 30., 36., 27.]]])\n",
    "assert np.allclose(a_stride2_same, expected_a_stride2_same), \"Stride 2 'same' forward mismatch!\"\n",
    "print(\"Stride 2 'same' forward matches expected.\")\n",
    "\n",
    "# Test stride = 3 with integer padding\n",
    "conv1d_stride3_int = Conv1dStrideAny(in_channels=in_channels, out_channels=out_channels, filter_size=3,\n",
    "                                      initializer=initializer, optimizer=optimizer, padding=1, stride=3)\n",
    "conv1d_stride3_int.W = w_stride_any\n",
    "conv1d_stride3_int.b = b_stride_any\n",
    "a_stride3_int = conv1d_stride3_int.forward(x_stride_any)\n",
    "print(\"Forward with stride 3 (integer padding=1):\\n\", a_stride3_int)\n",
    "# Padded input: [[0, 1, 2, 3, 4, 5, 6, 7, 0]], [[0, 8, 9, 10, 11, 12, 13, 14, 0]]\n",
    "# Output 1: [0+1+2, 3+4+5, 6+7+0] = [3, 12, 13]\n",
    "# Output 2: [0+8+9, 10+11+12, 13+14+0] = [17, 33, 27]\n",
    "expected_a_stride3_int = np.array([[[3., 12., 13.]], [[17., 33., 27.]]])\n",
    "assert np.allclose(a_stride3_int, expected_a_stride3_int), \"Stride 3 integer padding forward mismatch!\"\n",
    "print(\"Stride 3 integer padding forward matches expected.\")\n",
    "\n",
    "delta_a_stride2_same = np.ones_like(a_stride2_same)\n",
    "dX_stride2_same = conv1d_stride2_same.backward(delta_a_stride2_same)\n",
    "print(\"Backward dX with stride 2 ('same'):\\n\", dX_stride2_same.shape)\n",
    "assert dX_stride2_same.shape == x_stride_any.shape\n",
    "\n",
    "delta_a_stride3_int = np.ones_like(a_stride3_int)\n",
    "dX_stride3_int = conv1d_stride3_int.backward(delta_a_stride3_int)\n",
    "print(\"Backward dX with stride 3 (integer padding):\\n\", dX_stride3_int.shape)\n",
    "assert dX_stride3_int.shape == x_stride_any.shape\n",
    "\n",
    "print(\"Backward dW (stride 2):\\n\", conv1d_stride2_same.dW)\n",
    "print(\"Backward db (stride 2):\\n\", conv1d_stride2_same.db)\n",
    "print(\"Backward dW (stride 3):\\n\", conv1d_stride3_int.dW)\n",
    "print(\"Backward db (stride 3):\\n\", conv1d_stride3_int.db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125da2c",
   "metadata": {},
   "source": [
    "##### 【Problem 8 】 Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8faea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericg\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.1841 - loss: 2.1532 - val_accuracy: 0.2477 - val_loss: 1.8546\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2433 - loss: 1.8739\n",
      "\n",
      "Test Accuracy with Conv1D: 24.87%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape: (batch, 28, 28) is already good for Conv1D input (time steps=28, features=28)\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 2. One-hot encode labels\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "# 3. Define model\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28)),\n",
    "    Conv1D(16, kernel_size=3, padding='same', activation='relu'),\n",
    "    Conv1D(1, kernel_size=3, padding='same', activation='relu'),  # Final channel = 1\n",
    "    GlobalAveragePooling1D(),  # Reduce to (batch_size, 1)\n",
    "    Dense(10, activation='softmax')  # Join layer\n",
    "])\n",
    "\n",
    "# 4. Compile and train\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train_cat, epochs=1, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# 5. Evaluate\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_cat)\n",
    "print(f\"\\nTest Accuracy with Conv1D: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623fe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb07b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
